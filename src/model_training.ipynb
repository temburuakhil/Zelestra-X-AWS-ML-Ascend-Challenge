{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "def custom_score(y_true, y_pred):\n",
    "    \"\"\"Calculate the competition metric score.\"\"\"\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    return 100 * (1 - rmse)\n",
    "\n",
    "def train_test_split_data(X, y, test_size=0.2, random_state=42):\n",
    "    \"\"\"Split data into training and validation sets.\"\"\"\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "def get_base_models():\n",
    "    \"\"\"Get dictionary of base models for training.\"\"\"\n",
    "    return {\n",
    "        'RandomForest': RandomForestRegressor(random_state=42),\n",
    "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
    "        'XGBoost': XGBRegressor(random_state=42),\n",
    "        'LightGBM': LGBMRegressor(random_state=42),\n",
    "        'Ridge': Ridge()\n",
    "    }\n",
    "\n",
    "def optimize_xgboost(X_train, y_train, n_trials=100):\n",
    "    \"\"\"Optimize XGBoost hyperparameters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "            'tree_method': 'gpu_hist',  # Enable GPU acceleration\n",
    "            'predictor': 'gpu_predictor' # Use GPU predictor\n",
    "        }\n",
    "        \n",
    "        model = XGBRegressor(**params, random_state=42)\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, \n",
    "                               scoring='neg_root_mean_squared_error')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_lightgbm(X_train, y_train, n_trials=100):\n",
    "    \"\"\"Optimize LightGBM hyperparameters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.7, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 0, 10),\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 0, 10),\n",
    "            'device': 'gpu' # Enable GPU acceleration\n",
    "        }\n",
    "        \n",
    "        model = LGBMRegressor(**params, random_state=42, n_jobs=-1) # Use all available cores\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, \n",
    "                               scoring='neg_root_mean_squared_error')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "def optimize_random_forest(X_train, y_train, n_trials=50):\n",
    "    \"\"\"Optimize RandomForest hyperparameters using Optuna.\"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 25),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),\n",
    "            'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', 0.8, 1.0])\n",
    "        }\n",
    "        \n",
    "        model = RandomForestRegressor(**params, random_state=42, n_jobs=-1) # Use all available cores\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, \n",
    "                               scoring='neg_root_mean_squared_error')\n",
    "        return scores.mean()\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_params\n",
    "\n",
    "def train_models(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train multiple models and evaluate their performance.\"\"\"\n",
    "    models = get_base_models()\n",
    "    results = {}\n",
    "    \n",
    "    # No longer training individual models here as they will be tuned and trained within KFold\n",
    "    # This function will primarily be for initial checks or if individual model scores are needed outside the ensemble\n",
    "    # For the KFold loop, models are instantiated with optimized parameters.\n",
    "    \n",
    "    # For now, return empty results to proceed with KFold logic\n",
    "    return results\n",
    "\n",
    "def create_ensemble(models_dict, X_train, y_train, optimized_params_xgb, optimized_params_lgb, optimized_params_rf):\n",
    "    \"\"\"\n",
    "    Create a stacking ensemble from the best models with optimized hyperparameters.\n",
    "    \n",
    "    Args:\n",
    "        models_dict (dict): Dictionary of trained base models and their results (can be empty if models are re-instantiated)\n",
    "        X_train (pd.DataFrame): Training features\n",
    "        y_train (pd.Series): Training target\n",
    "        optimized_params_xgb (dict): Optimized hyperparameters for XGBoost\n",
    "        optimized_params_lgb (dict): Optimized hyperparameters for LightGBM\n",
    "        optimized_params_rf (dict): Optimized hyperparameters for RandomForest\n",
    "        \n",
    "    Returns:\n",
    "        sklearn.ensemble.StackingRegressor: Trained stacking ensemble model\n",
    "    \"\"\"\n",
    "    base_models = [\n",
    "        ('xgb', XGBRegressor(**optimized_params_xgb, random_state=42)),\n",
    "        ('lgb', LGBMRegressor(**optimized_params_lgb, random_state=42)),\n",
    "        ('rf', RandomForestRegressor(**optimized_params_rf, random_state=42))\n",
    "    ]\n",
    "\n",
    "    meta_learner = Ridge(alpha=0.1)\n",
    "\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=base_models,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    stacking_regressor.fit(X_train, y_train)\n",
    "    return stacking_regressor\n",
    "\n",
    "def analyze_feature_importance(model, feature_names, top_n=20):\n",
    "    \"\"\"Analyze and plot feature importance.\"\"\"\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        # Plot top features\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(data=importance_df.head(top_n), x='importance', y='feature')\n",
    "        plt.title(f'Top {top_n} Feature Importances')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/feature_importance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return importance_df\n",
    "    return None\n",
    "\n",
    "def create_shap_analysis(model, X_test, feature_names):\n",
    "    \"\"\"Create SHAP analysis for model interpretation.\"\"\"\n",
    "    if isinstance(model, (RandomForestRegressor, XGBRegressor, LGBMRegressor)):\n",
    "        # SHAP explainer should be fit on the model, not just estimators_[0]\n",
    "        # For StackingRegressor, it's more complex to get SHAP values directly for the ensemble.\n",
    "        # We can analyze the meta-learner or one of the base learners for interpretation.\n",
    "        # For simplicity, we'll use the first base model as before for now.\n",
    "        \n",
    "        # If you want SHAP for the full ensemble, you might need to use shap.KernelExplainer\n",
    "        # which is slower but model-agnostic.\n",
    "        \n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        shap.summary_plot(shap_values, X_test, feature_names=feature_names)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/shap_summary.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return explainer, shap_values\n",
    "    return None, None\n",
    "\n",
    "def main():\n",
    "    # Create plots directory\n",
    "    Path('plots').mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    from data_exploration import load_data\n",
    "    from feature_engineering import preprocess_data\n",
    "    \n",
    "    train_df, test_df, sample_submission = load_data()\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_columns = train_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    # Preprocess data\n",
    "    train_processed, test_processed = preprocess_data(train_df, test_df, categorical_columns)\n",
    "    # Preprocess test_df separately for feature creation\n",
    "    test_processed_for_features = preprocess_data(test_df.copy(), test_df.copy(), categorical_columns)[0]\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = train_processed.drop(['efficiency', 'id'], axis=1)\n",
    "    y = train_processed['efficiency']\n",
    "    \n",
    "    # --- Hyperparameter Tuning (run once before K-Fold if resources are limited) ---\n",
    "    print(\"\\n--- Starting Hyperparameter Optimization for Base Models ---\")\n",
    "    # For demonstration, limiting n_trials. Increase for better optimization.\n",
    "    optimized_params_xgb = optimize_xgboost(X, y, n_trials=200)\n",
    "    print(f\"Best XGBoost params: {optimized_params_xgb}\")\n",
    "    \n",
    "    optimized_params_lgb = optimize_lightgbm(X, y, n_trials=200)\n",
    "    print(f\"Best LightGBM params: {optimized_params_lgb}\")\n",
    "\n",
    "    optimized_params_rf = optimize_random_forest(X, y, n_trials=100) # RF is slower, so fewer trials\n",
    "    print(f\"Best RandomForest params: {optimized_params_rf}\")\n",
    "    \n",
    "    # --- K-Fold Cross-Validation for Final Model --- \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    oof_preds = np.zeros(X.shape[0])  # Out-of-fold predictions\n",
    "    test_preds = np.zeros(test_processed_for_features.shape[0]) # Test predictions\n",
    "    \n",
    "    # No longer storing all models, only the last one for feature importance/SHAP\n",
    "    last_trained_ensemble = None\n",
    "    last_trained_xgb_model = None\n",
    "    \n",
    "    val_scores = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X, y)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{kf.n_splits} ---\")\n",
    "        X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "        # Train base models and get results (train_models is now a placeholder, will be removed if not needed)\n",
    "        model_results = train_models(X_train, y_train, X_val, y_val) # This will be empty\n",
    "        \n",
    "        # Create and train ensemble for this fold with optimized hyperparameters\n",
    "        ensemble = create_ensemble(model_results, X_train, y_train, \n",
    "                                   optimized_params_xgb, optimized_params_lgb, optimized_params_rf)\n",
    "        \n",
    "        # Store the last trained ensemble model (for analysis outside the loop)\n",
    "        last_trained_ensemble = ensemble\n",
    "        \n",
    "        # Store the last trained XGBoost model from the ensemble for SHAP (as it's tree-based)\n",
    "        # Note: This assumes XGBoost is the first estimator in the ensemble list.\n",
    "        # If the order changes, this might need adjustment.\n",
    "        if 'xgb' in ensemble.named_estimators_:\n",
    "            last_trained_xgb_model = ensemble.named_estimators_['xgb']\n",
    "        \n",
    "        # Make out-of-fold predictions for validation set\n",
    "        oof_preds[val_index] = ensemble.predict(X_val)\n",
    "        \n",
    "        # Make predictions on test set (average over folds later)\n",
    "        test_preds += ensemble.predict(test_processed_for_features.drop(['id'], axis=1)) / kf.n_splits\n",
    "        \n",
    "        # Calculate and store validation score for this fold\n",
    "        fold_val_score = custom_score(y_val, oof_preds[val_index])\n",
    "        val_scores.append(fold_val_score)\n",
    "        print(f\"Fold {fold+1} Ensemble Validation Score: {fold_val_score:.2f}\")\n",
    "        \n",
    "    print(\"\\n=== K-Fold Cross-Validation Summary ===\")\n",
    "    print(f\"Mean Ensemble Validation Score: {np.mean(val_scores):.2f}\")\n",
    "    print(f\"Std Ensemble Validation Score: {np.std(val_scores):.2f}\")\n",
    "    \n",
    "    # Final predictions from averaged test predictions\n",
    "    final_test_predictions = test_preds\n",
    "    \n",
    "    # Clip predictions to ensure they are within the realistic efficiency bounds (0 to 1)\n",
    "    final_test_predictions = np.clip(final_test_predictions, 0, 1)\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'efficiency': final_test_predictions\n",
    "    })\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    # Optional: Feature importance and SHAP analysis using one of the trained models\n",
    "    if last_trained_xgb_model:\n",
    "        print(\"\\nPerforming Feature Importance and SHAP analysis...\")\n",
    "        feature_names = X.columns\n",
    "        \n",
    "        analyze_feature_importance(last_trained_xgb_model, feature_names)\n",
    "        # For SHAP, using the last validation fold's X_val as X_test_sample\n",
    "        create_shap_analysis(last_trained_xgb_model, X_val, feature_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
